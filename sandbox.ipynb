{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1: paroxytone, short ultima\n",
      "[(-3, 'πα'), (-2, 'πά'), (-1, 'πα^')]\n",
      "πά: paroxytone, short ultima\n",
      "Modified syllable positions: [(-3, 'πα'), (-2, 'πά^'), (-1, 'πα')]\n",
      "New version: παπά^πα\n",
      "παπά^πα^\n",
      "\n",
      "Case 2: paroxytone, long penultima\n",
      "[(-2, 'ά_'), (-1, 'α')]\n",
      "α: paroxytone with long acute\n",
      "Modified syllable positions: [(-2, 'ά'), (-1, 'α_')]\n",
      "New version: άα_\n",
      "ά_α_\n",
      "\n",
      "Case 3a: properispomenon\n",
      "[(-3, 'σω'), (-2, 'τῆ'), (-1, 'ρα')]\n",
      "ρα: properispomenon\n",
      "Modified syllable positions: [(-3, 'σω'), (-2, 'τῆ'), (-1, 'ρα^')]\n",
      "New version: σωτῆρα^\n",
      "σωτῆρα^\n",
      "\n",
      "Case 3b: proparoxytone\n",
      "[(-3, 'πά'), (-2, 'πα'), (-1, 'πα')]\n",
      "πα: proparoxytone\n",
      "Modified syllable positions: [(-3, 'πά'), (-2, 'πα'), (-1, 'πα^')]\n",
      "New version: πάπαπα^\n",
      "πάπαπα^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from class_macronizer import Macronizer\n",
    "\n",
    "macronizer = Macronizer()\n",
    "\n",
    "print('Case 1: paroxytone, short ultima')\n",
    "word = 'παπάπα^' # case 1\n",
    "word_after_accentuation_rules = macronizer.apply_accentuation_rules(word)\n",
    "print(word_after_accentuation_rules + '\\n')\n",
    "assert word_after_accentuation_rules == 'παπά^πα^'\n",
    "\n",
    "print('Case 2: paroxytone, long penultima')\n",
    "word = 'ά_α' # case 2\n",
    "word_after_accentuation_rules = macronizer.apply_accentuation_rules(word)\n",
    "assert word_after_accentuation_rules == 'ά_α_'\n",
    "print(word_after_accentuation_rules + '\\n')\n",
    "\n",
    "print('Case 3a: properispomenon')\n",
    "word = 'σωτῆρα' # case 3a\n",
    "word_after_accentuation_rules = macronizer.apply_accentuation_rules(word)\n",
    "assert word_after_accentuation_rules == 'σωτῆρα^'\n",
    "print(word_after_accentuation_rules + '\\n')\n",
    "\n",
    "print('Case 3b: proparoxytone')\n",
    "word = 'πάπαπα' # case 3b\n",
    "word_after_accentuation_rules = macronizer.apply_accentuation_rules(word)\n",
    "assert word_after_accentuation_rules == 'πάπαπα^'\n",
    "print(word_after_accentuation_rules + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "from class_macronizer import Macronizer\n",
    "from tests.anabasis import anabasis\n",
    "\n",
    "macronizer = Macronizer(unicode=False)\n",
    "\n",
    "macronized_anabasis = macronizer.macronize_text(anabasis)\n",
    "\n",
    "cProfile.run('macronizer.macronization_ratio(anabasis, macronized_anabasis, count_proper_names=True)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macronization took 0.01 seconds\n",
      "ἔχω κι^θά^ρα_ς ἀ^γα^θά_ς. νεα_νί^α_ς δ' εἰμὶ ἀ^ά_α^τος ὕδατι.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from class_macronizer import Macronizer\n",
    "\n",
    "macronizer = Macronizer(macronize_everything=True, debug=True)\n",
    "\n",
    "input = '''ἀάατος, ἀγαθὸς, καλὸς, ἀνήρ, νεανίας, ἰσχύς''' # why is ἰσχύς not treated as +1 unmacronized dichronon?\n",
    "input = \"ἔχω κιθάρας ἀγαθάς. νεανίας δ' εἰμὶ ἀάατος ὕδατι.\"\n",
    "output = macronizer.macronize_text(input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "normalize() argument 2 must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m macronizer = Macronizer(ifeellucky=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28minput\u001b[39m = [\u001b[33m'\u001b[39m\u001b[33mἱστάμεθα\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m output = \u001b[43mmacronizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacronize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/macronize-tlg/class_macronizer.py:139\u001b[39m, in \u001b[36mMacronizer.macronize\u001b[39m\u001b[34m(self, words)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Finally, apply accent rules\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.macronize_everything:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     results = {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_accentuation_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m results.items()} \u001b[38;5;66;03m# applied to v to make use of disambiguated dichronic ultima and penultima\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unicode:\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Optional: convert to Unicode format\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: macron_markup_to_unicode(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m results.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/macronize-tlg/class_macronizer.py:232\u001b[39m, in \u001b[36mMacronizer.apply_accentuation_rules\u001b[39m\u001b[34m(self, old_version)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m old_version:\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m old_version\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m old_version = \u001b[43mnormalize_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m old_version = macron_unicode_to_markup(old_version)\n\u001b[32m    234\u001b[39m new_version = old_version.replace(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).replace(\u001b[33m'\u001b[39m\u001b[33m^\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# this will be updated later\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/macronize-tlg/grc-utils/grc_utils/utils.py:110\u001b[39m, in \u001b[36mnormalize_word\u001b[39m\u001b[34m(word)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_word\u001b[39m(word):\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     normalized = \u001b[43municodedata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNFC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     tonos = oxia_to_tonos(normalized)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tonos\n",
      "\u001b[31mTypeError\u001b[39m: normalize() argument 2 must be str, not list"
     ]
    }
   ],
   "source": [
    "from class_macronizer import Macronizer\n",
    "\n",
    "macronizer = Macronizer(ifeellucky=False)\n",
    "\n",
    "input = ['ἱστάμεθα']\n",
    "output = macronizer.macronize(input)\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
